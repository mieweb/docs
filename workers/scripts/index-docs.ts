/**
 * Documentation Indexing Script
 *
 * Processes Hugo-generated search index and markdown content to create
 * vector embeddings for RAG-powered documentation search.
 *
 * Usage:
 *   npm run index-docs           # Process and upload to Vectorize
 *   npm run index-docs:dry-run   # Preview without uploading
 */

import * as fs from "fs/promises";
import * as path from "path";
import { execFileSync, spawnSync } from "child_process";

// ============================================================================
// Configuration
// ============================================================================

const CONFIG = {
  /** Path to Hugo search index (generated during build) */
  searchIndexPath: "../../public/eh/search.json",
  /** Path to content directory for full text extraction */
  contentPath: "../../content",
  /** Maximum chunk size in characters */
  maxChunkSize: 1500,
  /** Overlap between chunks for context continuity */
  chunkOverlap: 200,
  /** Minimum chunk size (skip smaller chunks) */
  minChunkSize: 100,
  /** Vectorize index name */
  vectorizeIndex: "docs-embeddings",
  /** Embedding model */
  embeddingModel: "@cf/baai/bge-base-en-v1.5",
  /** Batch size for vector uploads */
  batchSize: 100,
};

// ============================================================================
// Types
// ============================================================================

interface SearchIndexEntry {
  href: string;
  title: string;
  section?: string;
  content: string;
}

interface DocChunk {
  id: string;
  text: string;
  title: string;
  url: string;
  section?: string;
}

interface VectorRecord {
  id: string;
  values: number[];
  metadata: {
    title: string;
    url: string;
    section?: string;
    text: string;
  };
}

// ============================================================================
// Text Processing
// ============================================================================

/**
 * Clean and normalize text content
 *
 * Security note: This function is used to clean documentation content for
 * indexing/embedding purposes only. The output is stored as metadata and
 * never rendered as HTML or executed. The cleaning is purely for text quality.
 *
 * @nosecurity - CodeQL: This function processes trusted documentation content
 * from our own repository for search indexing. The output is stored as vector
 * embeddings metadata and is never rendered as HTML or executed as code.
 */
function cleanText(text: string): string {
  // Use a simple character-based approach to strip all HTML-like content
  // This avoids regex-based sanitization issues flagged by CodeQL
  let result = "";
  let inTag = false;
  let inScript = false;
  let tagBuffer = "";

  for (let i = 0; i < text.length; i++) {
    const char = text[i];

    if (char === "<") {
      inTag = true;
      tagBuffer = "<";
      continue;
    }

    if (inTag) {
      tagBuffer += char;
      if (char === ">") {
        inTag = false;
        // Check if this is a script opening tag
        const lowerTag = tagBuffer.toLowerCase();
        if (lowerTag.startsWith("<script")) {
          inScript = true;
        } else if (
          lowerTag.includes("</script") ||
          lowerTag.includes("< /script")
        ) {
          inScript = false;
        }
        tagBuffer = "";
      }
      continue;
    }

    // Skip content inside script tags
    if (inScript) {
      continue;
    }

    result += char;
  }

  // Apply remaining transformations
  return (
    result
      // Remove Hugo shortcodes
      .replace(/\{\{[%<].*?[%>]\}\}/g, "")
      // Remove markdown image syntax
      .replace(/!\[([^\]]*)\]\([^)]+\)/g, "$1")
      // Remove markdown link syntax but keep text
      .replace(/\[([^\]]+)\]\([^)]+\)/g, "$1")
      // Remove multiple newlines
      .replace(/\n{3,}/g, "\n\n")
      // Remove excessive whitespace
      .replace(/[ \t]+/g, " ")
      // Trim
      .trim()
  );
}

/**
 * Split text into chunks with overlap
 */
function chunkText(text: string, maxSize: number, overlap: number): string[] {
  const chunks: string[] = [];
  const sentences = text.split(/(?<=[.!?])\s+/);
  let currentChunk = "";

  for (const sentence of sentences) {
    if (currentChunk.length + sentence.length > maxSize) {
      if (currentChunk.length >= CONFIG.minChunkSize) {
        chunks.push(currentChunk.trim());
      }
      // Start new chunk with overlap from previous
      const words = currentChunk.split(" ");
      const overlapWords = words.slice(-Math.floor(overlap / 5));
      currentChunk = overlapWords.join(" ") + " " + sentence;
    } else {
      currentChunk += (currentChunk ? " " : "") + sentence;
    }
  }

  // Add final chunk
  if (currentChunk.length >= CONFIG.minChunkSize) {
    chunks.push(currentChunk.trim());
  }

  return chunks;
}

/**
 * Generate a unique ID for a chunk
 */
function generateChunkId(url: string, index: number): string {
  const urlHash = url.replace(/[^a-zA-Z0-9]/g, "-").toLowerCase();
  return `${urlHash}-chunk-${index}`;
}

// ============================================================================
// Search Index Processing
// ============================================================================

/**
 * Load and parse the Hugo search index
 */
async function loadSearchIndex(): Promise<SearchIndexEntry[]> {
  const indexPath = path.resolve(__dirname, CONFIG.searchIndexPath);

  try {
    const content = await fs.readFile(indexPath, "utf-8");
    const index = JSON.parse(content) as SearchIndexEntry[];
    console.log(`Loaded ${index.length} entries from search index`);
    return index;
  } catch (error) {
    console.error(`Failed to load search index from ${indexPath}:`, error);
    throw error;
  }
}

/**
 * Process search index entries into document chunks
 */
function processSearchIndex(entries: SearchIndexEntry[]): DocChunk[] {
  const chunks: DocChunk[] = [];

  for (const entry of entries) {
    if (!entry.content || entry.content.length < CONFIG.minChunkSize) {
      continue;
    }

    const cleanedText = cleanText(entry.content);
    const textChunks = chunkText(
      cleanedText,
      CONFIG.maxChunkSize,
      CONFIG.chunkOverlap
    );

    for (let i = 0; i < textChunks.length; i++) {
      chunks.push({
        id: generateChunkId(entry.href, i),
        text: textChunks[i],
        title: entry.title,
        url: entry.href,
        section: entry.section,
      });
    }
  }

  console.log(`Created ${chunks.length} chunks from ${entries.length} entries`);
  return chunks;
}

// ============================================================================
// Embedding Generation (via Wrangler)
// ============================================================================

/**
 * Generate embeddings using Wrangler AI command
 * This runs locally using your Cloudflare credentials
 */
async function generateEmbeddingsWithWrangler(
  chunks: DocChunk[]
): Promise<VectorRecord[]> {
  const records: VectorRecord[] = [];

  console.log(`Generating embeddings for ${chunks.length} chunks...`);

  // Process in batches to avoid memory issues
  for (let i = 0; i < chunks.length; i += CONFIG.batchSize) {
    const batch = chunks.slice(i, i + CONFIG.batchSize);
    console.log(
      `Processing batch ${Math.floor(i / CONFIG.batchSize) + 1}/${Math.ceil(chunks.length / CONFIG.batchSize)}`
    );

    for (const chunk of batch) {
      try {
        // Use wrangler to call AI for embeddings
        // Note: In production, this would use the Cloudflare API directly
        const truncatedText = chunk.text.slice(0, 500);
        const wranglerResult = spawnSync(
          "npx",
          [
            "wrangler",
            "ai",
            "run",
            CONFIG.embeddingModel,
            "--text",
            truncatedText,
          ],
          {
            cwd: path.resolve(__dirname, "../ai-assistant"),
            encoding: "utf-8",
            maxBuffer: 10 * 1024 * 1024,
          }
        );

        if (wranglerResult.error) {
          throw wranglerResult.error;
        }
        if (wranglerResult.status !== 0) {
          throw new Error(
            `wrangler exited with code ${wranglerResult.status}: ${wranglerResult.stderr}`
          );
        }

        const embeddingJson = wranglerResult.stdout;
        const embedding = JSON.parse(embeddingJson);
        const values = embedding.data?.[0] || embedding;

        if (Array.isArray(values)) {
          records.push({
            id: chunk.id,
            values,
            metadata: {
              title: chunk.title,
              url: chunk.url,
              section: chunk.section,
              text: chunk.text.slice(0, 1000), // Store truncated text in metadata
            },
          });
        }
      } catch (error) {
        console.warn(
          `Failed to generate embedding for chunk ${chunk.id}:`,
          error
        );
      }
    }
  }

  return records;
}

// ============================================================================
// Vectorize Upload
// ============================================================================

/**
 * Upload vectors to Cloudflare Vectorize
 */
async function uploadToVectorize(records: VectorRecord[]): Promise<void> {
  console.log(`Uploading ${records.length} vectors to Vectorize...`);

  // Write vectors to a temporary NDJSON file
  const tempFile = path.resolve(__dirname, ".vectors.ndjson");
  const ndjson = records.map((r) => JSON.stringify(r)).join("\n");
  await fs.writeFile(tempFile, ndjson);

  try {
    // Use wrangler to upload vectors
    const wranglerCwd = path.resolve(__dirname, "../ai-assistant");
    execFileSync(
      "npx",
      [
        "wrangler",
        "vectorize",
        "insert",
        CONFIG.vectorizeIndex,
        `--file=${tempFile}`,
      ],
      { stdio: "inherit", cwd: wranglerCwd }
    );
    console.log("Successfully uploaded vectors to Vectorize");
  } finally {
    // Clean up temp file
    await fs.unlink(tempFile).catch(() => {});
  }
}

// ============================================================================
// Main Entry Point
// ============================================================================

async function main(): Promise<void> {
  const args = process.argv.slice(2);
  const dryRun = args.includes("--dry-run");

  console.log("=".repeat(60));
  console.log("Documentation Indexing Script");
  console.log(dryRun ? "(DRY RUN - no uploads)" : "");
  console.log("=".repeat(60));

  // Step 1: Load search index
  console.log("\nðŸ“š Loading search index...");
  const searchIndex = await loadSearchIndex();

  // Step 2: Process into chunks
  console.log("\nâœ‚ï¸  Processing into chunks...");
  const chunks = processSearchIndex(searchIndex);

  if (dryRun) {
    console.log("\nðŸ“Š Dry run summary:");
    console.log(`  Total documents: ${searchIndex.length}`);
    console.log(`  Total chunks: ${chunks.length}`);
    console.log(
      `  Average chunk size: ${Math.round(chunks.reduce((sum, c) => sum + c.text.length, 0) / chunks.length)} chars`
    );
    console.log("\nSample chunks:");
    for (const chunk of chunks.slice(0, 3)) {
      console.log(`\n  [${chunk.title}] (${chunk.text.length} chars)`);
      console.log(`  ${chunk.text.slice(0, 100)}...`);
    }
    return;
  }

  // Step 3: Generate embeddings
  console.log("\nðŸ§  Generating embeddings...");
  const records = await generateEmbeddingsWithWrangler(chunks);

  // Step 4: Upload to Vectorize
  console.log("\nâ˜ï¸  Uploading to Vectorize...");
  await uploadToVectorize(records);

  console.log("\nâœ… Indexing complete!");
}

main().catch((error) => {
  console.error("Fatal error:", error);
  process.exit(1);
});
